{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "qtvaxj63ozz5e3nuzju5",
   "authorId": "96087873734",
   "authorName": "FAHAD_WEST",
   "authorEmail": "fahad.azeem@snowflake.com",
   "sessionId": "771c5666-5b1c-4fd5-bbf3-ab0c8eea5af3",
   "lastEditTime": 1736890716071
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d519415f-6a1e-4d04-918f-6f0eb0c96399",
   "metadata": {
    "collapsed": false,
    "name": "_0_HEADLINE",
    "resultHeight": 0
   },
   "source": [
    "# Titanic - Machine Learning from Disaster\n",
    "Source:  \n",
    "https://www.kaggle.com/competitions/titanic\n",
    "\n",
    "### The Challenge\n",
    "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
    "\n",
    "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "\n",
    "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n",
    "\n",
    "In this challenge, we ask you to build a predictive model that answers the question: “what sorts of people were more likely to survive?” using passenger data (i.e. name, age, gender, socio-economic class, etc).\n",
    "\n",
    "### What Data Will I Use in This Competition?\n",
    "In this competition, you have access to a Snowflake table called **PASSENGERS**.  \n",
    "This table contains the **PASSENGER_ID** and a label-column **SURVIVED** which indicates whether a passenger sruvived or not.  \n",
    "We know the survival status for 891 passengers but the status for the remaining 418 passengers is unknown and therefore missing in that table.\n",
    "\n",
    "In addition, your team of smart datascientists already registered a couple of features in your **Snowflake Feature Store**.  \n",
    "This feature store includes information about the name, age, gender, socio-economic class, etc. \n",
    "\n",
    "Use your datascience expertise to:\n",
    "* Explore the existing Data\n",
    "* Develop and Register new Features in the **Feature Store**\n",
    "* Train and Register a Machine Learning Model in the **Model Registry**\n",
    "* Create Scores for passengers with unknown survival status\n",
    "\n",
    "### Evaluation\n",
    "#### Goal\n",
    "It is your job to predict if a passenger survived the sinking of the Titanic or not.  \n",
    "For each passenger where the survival status is unknown, you must predict a 0 (died) or 1 (survived).\n",
    "\n",
    "#### Metric\n",
    "Your score is the percentage of passengers you correctly predict. This is known as accuracy."
   ]
  },
  {
   "cell_type": "code",
   "id": "918bfb97-8d4b-474b-9bad-a047465c0761",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "collapsed": false,
    "resultHeight": 1895,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "!pip install 'snowflake-ml-python==1.7.2'\n!pip install tabulate",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "28b4abd5-f700-44c7-adb3-870f026473c3",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "collapsed": false,
    "resultHeight": 38
   },
   "outputs": [],
   "source": "from snowflake.ml import version\nprint(version.VERSION)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1409aa3d-3e66-43e9-b832-61253bcc14ee",
   "metadata": {
    "collapsed": false,
    "name": "_1_IMPORTS1",
    "resultHeight": 0
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4049a079-6d27-4d75-bb4d-ea763ae20052",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_1_IMPORTS2",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Snowpark Imports\nfrom snowflake.snowpark.context import get_active_session\nimport snowflake.snowpark.functions as F\nfrom snowflake.snowpark.functions import col, lit, when\n\n# Snowpark ML\nfrom snowflake.ml.modeling.impute import SimpleImputer\nfrom snowflake.ml.modeling.preprocessing import OrdinalEncoder, OneHotEncoder, Normalizer\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.xgboost import XGBClassifier\nfrom snowflake.ml.modeling.model_selection import GridSearchCV\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.feature_store import FeatureStore, FeatureView, CreationMode\nfrom snowflake.cortex import Complete\n\n# Other Imports\nimport matplotlib.pyplot as plt\nimport json\nimport streamlit as st\nimport plotly.express as px\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
   "cell_type": "markdown",
   "id": "333c75c1-0fa8-4bf0-a954-5c21237cd5d4",
   "metadata": {
    "collapsed": false,
    "name": "_2_SETUP1",
    "resultHeight": 0
   },
   "source": [
    "# 2 - Set Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496c9046-0510-44d5-96ef-03c354504ee2",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_2_SETUP2",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Retrieve the Session\n",
    "session = get_active_session()\n",
    "\n",
    "# Set context\n",
    "session.use_schema('KAGGLE_TITANIC_CHALLENGE.DEVELOPMENT')\n",
    "\n",
    "# Create reference to Feature Store\n",
    "fs = FeatureStore(\n",
    "    session=session, \n",
    "    database=\"KAGGLE_TITANIC_CHALLENGE\", \n",
    "    name=\"DEVELOPMENT\", \n",
    "    default_warehouse=\"COMPUTE_WH\",\n",
    "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n",
    ")\n",
    "\n",
    "# Create reference to Model Registry\n",
    "model_registry = Registry(\n",
    "    session=session, \n",
    "    database_name=session.get_current_database(), \n",
    "    schema_name=session.get_current_schema()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058c878f-f2eb-43bf-8a82-961a6b9a744e",
   "metadata": {
    "collapsed": false,
    "name": "_3_EXPLORATION1",
    "resultHeight": 0
   },
   "source": [
    "# 3 - Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfc4be5-5932-4aa1-9596-e255060e0037",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_3_EXPLORE_PASSENGERS",
    "resultHeight": 329
   },
   "outputs": [],
   "source": [
    "passengers = session.table('PASSENGER')\n",
    "passengers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218a3f56-17a7-4669-9f86-06def3366ddd",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_3_DISCOVER_FEATURES",
    "resultHeight": 868
   },
   "outputs": [],
   "source": [
    "# Discover available entites, feature views and features\n",
    "print('Entities:')\n",
    "fs.list_entities().show()\n",
    "entity = fs.get_entity(name=\"PASSENGER\")\n",
    "\n",
    "print('Feature Views:')\n",
    "fs.list_feature_views().show()\n",
    "kaggle_fv = fs.get_feature_view('PASSENGER_KAGGLE_FEATURES','V1')\n",
    "\n",
    "print('Features in PASSENGER_KAGGLE_FEATURES:')\n",
    "pd.DataFrame(list(kaggle_fv.feature_descs.items()), columns=['FEATURE_NAME', 'DESCRIPTION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06890236-8044-4d9a-9fc8-fa70252203a5",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_3_VIEW_EXISTING_FEATURES",
    "resultHeight": 351
   },
   "outputs": [],
   "source": [
    "# Retrieve existing features for your data\n",
    "titanic_df = fs.retrieve_feature_values(passengers, [kaggle_fv])\n",
    "titanic_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6e540c-8b37-45e0-a22c-2fa144a23ef1",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_3_DESCRIBE1",
    "resultHeight": 252
   },
   "outputs": [],
   "source": [
    "# Statistics for all columns in dataset (where SURVIVED is not missing)\n",
    "train_df_summary = titanic_df.describe().to_pandas()\n",
    "train_df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ddd51-77bd-4fe3-9d48-1e90b50b6eb5",
   "metadata": {
    "collapsed": false,
    "name": "_3_LLM_SUMMARY1",
    "resultHeight": 0
   },
   "source": [
    "That's a ton of information! What do these statistics mean? What should I do?  \n",
    "Let's ask an LLM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb20962f-f54f-47b0-986d-c15025cf0dbc",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_3_LLM_SUMMARY2",
    "resultHeight": 713
   },
   "outputs": [],
   "source": "#llm = 'mixtral-8x7b'\nllm = 'llama3.1-70b'\n\nprompt = f\"\"\"\nI used Snowparks describe function to calculate count, mean, stddev, min and max per column.\nPASSENGER_ID is unique and identifies each row. I want to predict the variable SURVIVED.\nWhat feature engineering steps should I perform before building a machine learning model?\nMake sure to explain your recommendations based on the data.\n{train_df_summary.to_markdown()}\n\"\"\"\n\nresponse = Complete(llm, prompt)\nst.markdown(response)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee6fce7-c760-46ef-bc41-a3f497ab6339",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_3_FEAT_SCALING_OUTLIER",
    "resultHeight": 596
   },
   "outputs": [],
   "source": [
    "# Create a figure and a set of subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n",
    "\n",
    "# Create a boxplot in the first subplot\n",
    "ax1.boxplot(titanic_df[['AGE']].dropna().to_pandas())\n",
    "ax1.set_title('Boxplot of AGE')\n",
    "ax1.set_ylabel('Values')\n",
    "\n",
    "# Create a boxplot in the second subplot\n",
    "ax2.boxplot(titanic_df[['FARE']].dropna().to_pandas())\n",
    "ax2.set_title('Boxplot of FARE')\n",
    "ax2.set_ylabel('Values')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b58db8-fe6b-41f7-af03-d83ba870d79d",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_3_CAT_BARPLOTS",
    "resultHeight": 746
   },
   "outputs": [],
   "source": [
    "# Visualize variables in relation to survival probability\n",
    "col1, col2 = st.columns(2)\n",
    "col1.bar_chart(titanic_df.group_by('SEX').agg(F.avg('SURVIVED').as_('SURVIVAL_PROB')).to_pandas(), x='SEX', y='SURVIVAL_PROB')\n",
    "col2.bar_chart(titanic_df.group_by('EMBARKED').agg(F.avg('SURVIVED').as_('SURVIVAL_PROB')).to_pandas(), x='EMBARKED', y='SURVIVAL_PROB')\n",
    "col1.bar_chart(titanic_df.group_by('PCLASS').agg(F.avg('SURVIVED').as_('SURVIVAL_PROB')).to_pandas(), x='PCLASS', y='SURVIVAL_PROB')\n",
    "col2.bar_chart(titanic_df.group_by('SIB_SP').agg(F.avg('SURVIVED').as_('SURVIVAL_PROB')).to_pandas(), x='SIB_SP', y='SURVIVAL_PROB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84adf20b-219c-48a0-b476-227da4d5e0a8",
   "metadata": {
    "collapsed": false,
    "name": "_4_FEATURE_ENGINEERING",
    "resultHeight": 74
   },
   "source": [
    "# 4 - Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a759f98-1d8f-4438-8e17-0bdb11d11613",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_4_FAMILY_SIZE",
    "resultHeight": 645
   },
   "outputs": [],
   "source": [
    "# Features: Family Size\n",
    "# We can imagine that large families will have more difficulties to evacuate, looking for theirs sisters/brothers/parents during the evacuation. \n",
    "# So, we to create a \"FAMILY_SIZE\" feature which is the sum of SIBSP , PARCH and 1 (including the passenger).\n",
    "titanic_df = titanic_df.with_column('FAM_SIZE', col('SIB_SP') + col('PARCH') + 1)\n",
    "\n",
    "# We further can create groups based on the FAMILY_SIZE\n",
    "titanic_df = titanic_df.with_column('FAM_SIZE_CATEGORY', \n",
    "                                when(col('FAM_SIZE') == 1, 'SINGLE')\n",
    "                                .when(col('FAM_SIZE') == 2, 'COUPLE')\n",
    "                                .when(col('FAM_SIZE') >= 5, 'LARGE_FAMILY')\n",
    "                                .otherwise('NORMAL_FAMILY'))\n",
    "\n",
    "titanic_df[['FAM_SIZE','FAM_SIZE_CATEGORY','SURVIVED']].show(3)\n",
    "\n",
    "# Analyze family sizes for training data\n",
    "analysis_df = titanic_df.filter(col('SURVIVED').is_not_null()).group_by('FAM_SIZE_CATEGORY')\n",
    "analysis_df = analysis_df.agg(F.count('FAM_SIZE_CATEGORY').as_('COUNT'), F.avg('SURVIVED').as_('SURVIVAL_PROB')).order_by('SURVIVAL_PROB')\n",
    "analysis_df = analysis_df.to_pandas()\n",
    "\n",
    "fig = px.scatter(analysis_df, x=\"COUNT\", y=\"SURVIVAL_PROB\", size=\"COUNT\", color=\"FAM_SIZE_CATEGORY\",\n",
    "                 hover_name=\"FAM_SIZE_CATEGORY\", title=\"Frequency of Family Sizes and Their Relation to Survival Probability\",\n",
    "                 labels={\"COUNT\": \"Count of Titles\", \"SURVIVAL_PROB\": \"Survival Probability\"},\n",
    "                 size_max=60)\n",
    "\n",
    "st.plotly_chart(fig, use_container_width=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fef19d-c7a3-431e-90af-bc23c3e6febb",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_4_TITLE",
    "resultHeight": 645
   },
   "outputs": [],
   "source": [
    "# Retrieve title from name\n",
    "titanic_df = titanic_df.with_column('TITLE',F.trim(F.split(F.split(col('NAME'), F.lit(','))[1],F.lit('.'))[0]))\n",
    "# Uppercase the title\n",
    "titanic_df = titanic_df.with_column('TITLE',F.upper(col('TITLE')))\n",
    "titanic_df[['NAME','TITLE','SURVIVED']].show(3)\n",
    "\n",
    "# Combine rare titles\n",
    "rare_titles = ['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona','Ms','Mme','Mlle']\n",
    "rare_titles = [title.upper() for title in rare_titles]\n",
    "titanic_df = titanic_df.with_column('TITLE', when(col('TITLE').isin(rare_titles), 'Rare').otherwise(col('TITLE')))\n",
    "\n",
    "# Analyze titles\n",
    "analysis_df = titanic_df.filter(col('SURVIVED').is_not_null()).group_by('TITLE')\n",
    "analysis_df = analysis_df.agg(F.count('TITLE').as_('COUNT'), F.avg('SURVIVED').as_('SURVIVAL_PROB')).order_by('SURVIVAL_PROB')\n",
    "analysis_df = analysis_df.to_pandas()\n",
    "\n",
    "fig = px.scatter(analysis_df, x=\"COUNT\", y=\"SURVIVAL_PROB\", size=\"COUNT\", color=\"TITLE\",\n",
    "                 hover_name=\"TITLE\", title=\"Frequency of Titles and Their Relation to Survival Probability\",\n",
    "                 labels={\"COUNT\": \"Count of Titles\", \"SURVIVAL_PROB\": \"Survival Probability\"},\n",
    "                 size_max=60)\n",
    "\n",
    "st.plotly_chart(fig, use_container_width=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed416eb2-c296-44eb-ab36-46aa369eedab",
   "metadata": {
    "collapsed": false,
    "name": "_4_LLM_FEATURE_DESC1",
    "resultHeight": 0
   },
   "source": [
    "We are lazy so let's use an LLM to generate the feature descriptions based on the underlying SQL transformations of our Snowpark DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a37112e-c8a3-4a73-b17d-1b837f50f64e",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "python",
    "name": "_4_LLM_FEATURE_DESC2",
    "resultHeight": 296
   },
   "outputs": [],
   "source": "llm = 'llama3.1-70b'\n\nprompt = f\"\"\"\nYou are provided with a SQL Query that derives features from existing columns.\nDescribe the features FAM_SIZE, FAM_SIZE_CATEGORY and TITLE.\nThe descriptions will be stored in a feature store, so make sure to return a JSON where the feature name is the key and the description is the value.\n{titanic_df.queries['queries'][0]}\n\"\"\"\nllm_response = Complete(llm, prompt)\n\nfeature_descriptions = json.loads(llm_response.split('```')[1])\n\nfor key in feature_descriptions:\n    feature_descriptions[key] = feature_descriptions[key].replace(\"'\", '')\nfeature_descriptions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533e251d-0007-46ba-9e47-acacacc4ff5c",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_4_FEAT_STORE_TITLE_FAM_SIZE",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Create Feature View with Custom Features\ncustom_fv = FeatureView(\n    name=\"PASSENGER_CUSTOM_FEATURES\", \n    entities=[entity],\n    feature_df=titanic_df[['PASSENGER_ID','TITLE','FAM_SIZE','FAM_SIZE_CATEGORY']],\n    refresh_freq=\"1 minute\",\n    desc=\"Custom Passenger Features\")\n\n# Add descriptions for some features\ncustom_fv = custom_fv.attach_feature_desc(feature_descriptions)\n\ncustom_fv = fs.register_feature_view(\n    feature_view=custom_fv, \n    version=\"V1\", \n    block=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659269fa-ba59-470d-aba2-79048d42c558",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_4_TRAIN_TEST_SPLIT",
    "resultHeight": 374
   },
   "outputs": [],
   "source": [
    "spine_df = session.table('PASSENGER')\n",
    "\n",
    "spine_df_train = spine_df.filter(col('SURVIVED').is_not_null())\n",
    "print(f'Train dataset has {spine_df_train.count()} passengers.')\n",
    "spine_df_train.show(3)\n",
    "\n",
    "spine_df_test = spine_df.filter(col('SURVIVED').is_null())\n",
    "print(f'Test dataset has {spine_df_test.count()} passengers.')\n",
    "spine_df_test.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3a110-54eb-4aa0-8bbf-281239ed919d",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_4_GENERATE_TRAIN_DATASET",
    "resultHeight": 194
   },
   "outputs": [],
   "source": "# Generate the training dataset by retrieving the features\ntraining_dataset = fs.generate_dataset(\n    name=\"TITANIC_TRAINING_DATASET_NEW\",\n    spine_df=spine_df_train,\n    features=[kaggle_fv,custom_fv],\n    spine_label_cols=[\"SURVIVED\"],\n    desc=\"Training Data to train model to predict whether a passenger survived.\"\n)\n\n# Retrieve a Snowpark DataFrame from the registered Dataset\ntraining_dataset_df = training_dataset.read.to_snowpark_dataframe().cache_result()\ntraining_dataset_df.show(3)"
  },
  {
   "cell_type": "code",
   "id": "228dc985-8a45-45b4-8017-1f639a68bf59",
   "metadata": {
    "language": "python",
    "name": "_4_CHECK_GPUS",
    "collapsed": false,
    "resultHeight": 60
   },
   "outputs": [],
   "source": "import torch\n\n# Get the list of GPUs\nif torch.cuda.is_available():\n    # Get the number of GPUs\n    num_gpus = torch.cuda.device_count()\n\n    print(f'{num_gpus} GPU Device(s) Found')\n    # Print the list of GPUs\n    for i in range(num_gpus):\n        print(\"Name:\", torch.cuda.get_device_name(i), \"  Index:\", i)\nelse:\n    print(\"No GPU available\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "137ff3dc-0429-47a9-a420-d6d9f8f45d6f",
   "metadata": {
    "collapsed": false,
    "name": "_5_MODELLING",
    "resultHeight": 0
   },
   "source": [
    "# 5 - Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a42bb9-ce39-4efa-a0e0-50891b7050af",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_5_SWITCH_COMPUTE1",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Switch to a larger warehouse to speed up training\n#session.use_warehouse('TRAIN_WH')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9accec4a-0df9-497e-aa69-899dcb28b37c",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_5_PRE_PROCESSING_PIPELINE",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "\n# DROP unused variables\n# PASSENGER_ID -> just an artificial ID with no predictive value\n# NAME -> people won't survive just because of their name and we extracted the title already\n# CABIN -> too many missing values\n# TICKET -> doesn't contain valuable information in its current form since it's a unique value per customer\ntraining_dataset_df = training_dataset_df.drop(['PASSENGER_ID','NAME','CABIN','TICKET'])\n\n# Impute Age by mean\nsi_age =  SimpleImputer(\n    input_cols=['AGE','FARE'], \n    output_cols=['AGE_IMP','FARE_IMP'],\n    strategy='mean',\n    drop_input_cols=True\n)\n\n# Normalize Fare and Age\nnorm = Normalizer(\n    input_cols=['AGE_IMP','FARE_IMP'],\n    output_cols=['AGE_IMP_NORM','FARE_IMP_NORM'],\n    drop_input_cols=True\n)\n\n# One-Hot-Encoding of categorical features\nohe = OneHotEncoder(\n    input_cols=['SEX','EMBARKED','TITLE','FAM_SIZE_CATEGORY'], \n    output_cols=['SEX','EMBARKED','TITLE','FAM_SIZE_CATEGORY'],\n    drop_input_cols=True\n)\n\n\n\n# Build the pipeline\nmodel_pipeline = Pipeline(\n    steps=[\n        (\"IMPUTE\",si_age),\n        (\"NORMALIZE\",norm),\n        (\"ONE_HOT_ENCODE\",ohe),\n        #(\"GRIDSEARCH_XGBOOST\",grid_search)\n    ]\n)\n\n# Fit the pipeline to the training data\ntransformed_data = model_pipeline.fit_transform(training_dataset_df)"
  },
  {
   "cell_type": "markdown",
   "id": "993f0807-f491-4b71-ac0e-7db98d75637d",
   "metadata": {
    "name": "_5_MODELLING_PIPELINE_BACKUP",
    "collapsed": true,
    "resultHeight": 1279
   },
   "source": "\n# DROP unused variables\n# PASSENGER_ID -> just an artificial ID with no predictive value\n# NAME -> people won't survive just because of their name and we extracted the title already\n# CABIN -> too many missing values\n# TICKET -> doesn't contain valuable information in its current form since it's a unique value per customer\ntraining_dataset_df = training_dataset_df.drop(['PASSENGER_ID','NAME','CABIN','TICKET'])\n\n# Impute Age by mean\nsi_age =  SimpleImputer(\n    input_cols=['AGE','FARE'], \n    output_cols=['AGE_IMP','FARE_IMP'],\n    strategy='mean',\n    drop_input_cols=True\n)\n\n# Normalize Fare and Age\nnorm = Normalizer(\n    input_cols=['AGE_IMP','FARE_IMP'],\n    output_cols=['AGE_IMP_NORM','FARE_IMP_NORM'],\n    drop_input_cols=True\n)\n\n# One-Hot-Encoding of categorical features\nohe = OneHotEncoder(\n    input_cols=['SEX','EMBARKED','TITLE','FAM_SIZE_CATEGORY'], \n    output_cols=['SEX','EMBARKED','TITLE','FAM_SIZE_CATEGORY'],\n    drop_input_cols=True\n)\n\n# Define the XGBoost model (incl. Hyperparameter Tuning)\nlabel_cols = ['SURVIVED']\noutput_cols = ['SURVIVED_PREDICTION']\n\ngrid_search = GridSearchCV(\n    estimator=XGBClassifier(random_state=42),\n    param_grid={\n        'n_estimators':[10, 50, 100],\n        'max_depth': [2,4,8],\n        'learning_rate':[.01, .03, .1],\n    },\n    n_jobs = -1,\n    scoring=\"accuracy\",\n    label_cols=label_cols,\n    output_cols=output_cols\n)\n\n# Build the pipeline\nmodel_pipeline = Pipeline(\n    steps=[\n        (\"IMPUTE\",si_age),\n        (\"NORMALIZE\",norm),\n        (\"ONE_HOT_ENCODE\",ohe),\n        (\"GRIDSEARCH_XGBOOST\",grid_search)\n    ]\n)\n\n# Fit the pipeline to the training data\nfitted_pipeline = model_pipeline.fit(training_dataset_df)"
  },
  {
   "cell_type": "code",
   "id": "deb9f6ba-e961-4381-8b5a-0d4d5f1c667d",
   "metadata": {
    "language": "python",
    "name": "_5_MODELLING_NOT_DISTRIBUTED",
    "collapsed": false,
    "resultHeight": 405
   },
   "outputs": [],
   "source": "# Define the XGBoost model (incl. Hyperparameter Tuning)\nlabel_cols = ['SURVIVED']\noutput_cols = ['SURVIVED_PREDICTION']\n\ngrid_search = GridSearchCV(\n    estimator=XGBClassifier(random_state=42),\n    param_grid={\n        'n_estimators':[10, 50, 100],\n        'max_depth': [2,4,8],\n        'learning_rate':[.01, .03, .1],\n    },\n    n_jobs = -1,\n    scoring=\"accuracy\",\n    label_cols=label_cols,\n    output_cols=output_cols\n)\n\nreturn_model = grid_search.fit(transformed_data)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5bb89bc2-901e-418a-a294-afc8ef56e571",
   "metadata": {
    "language": "python",
    "name": "_5_MODELLING_DISTRIBUTED",
    "collapsed": false,
    "resultHeight": 3453
   },
   "outputs": [],
   "source": "# Import necessary classes for Snowflake Container Runtime training\nfrom snowflake.ml.modeling.distributors.xgboost import XGBEstimator, XGBScalingConfig\n#from sklearn.model_selection import GridSearchCV as GridSearchCVsk \nfrom snowflake.ml.data.data_connector import DataConnector\n\n# Set up the scaling configuration for multi-GPU usage\nscaling_config = XGBScalingConfig(\n    num_workers=-1,            # Use all available workers\n    num_cpu_per_worker=-1,     # Use all available CPU cores per worker\n    use_gpu=True               # Enable GPU for training\n)\n\n# Define the XGBEstimator for training\ndst_estimator = XGBEstimator(\n    n_estimators=100,                     # Number of trees\n    objective='binary:logistic',         # Objective function for regression\n    scaling_config=scaling_config         # Use GPU and multi-worker scaling\n)\n\nlabel_cols = 'SURVIVED'\noutput_cols = 'SURVIVED_PREDICTION'\n\ninput_cols = [col for col in transformed_data.columns]\ntrain_data_connector = DataConnector.from_dataframe(transformed_data)\n\nreturn_dst_model = dst_estimator.fit(train_data_connector, input_cols=input_cols,label_col=label_cols)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "86c9d374-3373-4521-af68-df887902c7ac",
   "metadata": {
    "language": "python",
    "name": "cell6",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "# Retrieving features for the test data\n'''test_dataset_df = fs.retrieve_feature_values(\n    spine_df=spine_df_test, \n    features=[kaggle_fv,custom_fv], \n    exclude_columns=['SURVIVED']\n)'''\n\n\n#test_data_connector.to_pandas()\ny_predit = return_dst_model.predict(train_data_connector)\n#y_predit\n\n#train_data_connector.to_pandas()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bd8209-6e75-4ecd-aa73-39a9cbdda16e",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_5_SWITCH_COMPUTE2",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Switch back to smaller warehouse to save ressources\n#session.use_warehouse('COMPUTE_WH')"
  },
  {
   "cell_type": "markdown",
   "id": "cabd2960-a911-47f2-9a3c-28ca5a29d477",
   "metadata": {
    "collapsed": false,
    "name": "_6_MODEL_EVALUATION",
    "resultHeight": 74
   },
   "source": [
    "# 6 - Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fb64d9-bf79-4c8e-b28a-ad6d820461d3",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_6_FEATURE_IMPORTANCE",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "model_object = fitted_pipeline.to_sklearn().named_steps['GRIDSEARCH_XGBOOST']\n",
    "\n",
    "# Get the Feature Importance\n",
    "feature_importance = pd.DataFrame(\n",
    "    zip(model_object.best_estimator_.feature_names_in_, model_object.best_estimator_.feature_importances_),\n",
    "    columns=['FEATURE_NAME','IMPORTANCE']\n",
    ").sort_values('IMPORTANCE', ascending=False)\n",
    "\n",
    "fig = px.bar(feature_importance, x='FEATURE_NAME', y='IMPORTANCE')\n",
    "st.plotly_chart(fig)"
   ]
  },
  {
   "cell_type": "code",
   "id": "91295b59-d60c-4972-acae-fabb18c6966f",
   "metadata": {
    "language": "python",
    "name": "cell4",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "#type(model_object)\nimport joblib\n\njoblib.dump()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da094c3b-796a-4cf5-adb1-a1e653c29aed",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_6_PLOT_PARAMETERS",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Build a dataframe for the gridsearch results\n",
    "gs_results = model_object.cv_results_\n",
    "n_estimators_val = []\n",
    "learning_rate_val = []\n",
    "max_depth_val = []\n",
    "for param_dict in gs_results[\"params\"]:\n",
    "    n_estimators_val.append(param_dict[\"n_estimators\"])\n",
    "    learning_rate_val.append(param_dict[\"learning_rate\"])\n",
    "    max_depth_val.append(param_dict[\"max_depth\"])\n",
    "accuracy_val = gs_results[\"mean_test_score\"]\n",
    "\n",
    "gs_results_df = pd.DataFrame(data={\n",
    "    \"n_estimators\":n_estimators_val,\n",
    "    \"learning_rate\":learning_rate_val,\n",
    "    \"max_depth\":max_depth_val,\n",
    "    \"accuracy\":accuracy_val})\n",
    "\n",
    "print(f'Number of Models: {len(gs_results_df)}')\n",
    "print('Best Parameter Configuration:')\n",
    "model_object.best_params_\n",
    "print(f'Accuracy of best Model: {model_object.best_score_}')\n",
    "\n",
    "# Create a 3D scatter plot to visualize impact of parameters on accuracy\n",
    "fig = px.scatter_3d(gs_results_df, x='learning_rate', y='n_estimators', z='max_depth', color='accuracy',\n",
    "                    labels={'accuracy': 'accuracy'})\n",
    "\n",
    "# Update the layout to increase the size of the chart\n",
    "fig.update_layout(\n",
    "    width=1000,  # Set the desired width\n",
    "    height=800   # Set the desired height\n",
    ")\n",
    "\n",
    "# Display the chart\n",
    "st.plotly_chart(fig)"
   ]
  },
  {
   "cell_type": "code",
   "id": "d3967d7b-0e76-42c9-b798-437f5ebc852f",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "from joblib import dump, load\n\ndump(model_object, 'model.joblib')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dbd54739-6f05-4fa2-ad31-b8c0df8ee837",
   "metadata": {
    "language": "python",
    "name": "cell5",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "session.file.put(\"model.joblib\",\"@KAGGLE_TITANIC_CHALLENGE.DEVELOPMENT.KAGGLE_SUBMISSION\", auto_compress= False)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e949e216-35c8-4a12-8ce5-8f984ef2ca28",
   "metadata": {
    "collapsed": false,
    "name": "_7_REGISTER_MODEL1",
    "resultHeight": 74
   },
   "source": [
    "# 7 - Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158ffce2-3916-4505-bf4b-691e76ecee70",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_7_REGISTER_MODEL2",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Register new model version\nregistered_model = model_registry.log_model(\n    fitted_pipeline,\n    model_name=\"TITANIC_SURVIVAL_MODEL_SPCS_1\",\n    comment=\"Model trained using GridsearchCV in Snowpark to predict survival of Titanic passengers.\",\n    metrics={\"accuracy\": model_object.best_score_},\n    conda_dependencies=['xgboost'],\n    version_name='v1'\n)\n\n# View available models\nmodel_registry.show_models()\n\n# View available versions\nmodel_registry.get_model('TITANIC_SURVIVAL_MODEL_SPCS_1').show_versions()"
  },
  {
   "cell_type": "code",
   "id": "39779d71-0e8a-4143-8870-9ce753591e54",
   "metadata": {
    "language": "python",
    "name": "_7_REGISTER_MODEL_DIST"
   },
   "outputs": [],
   "source": "# Register new model version\nregistered_model = model_registry.log_model(\n    return_dst_model,\n    model_name=\"TITANIC_SURVIVAL_MODEL_DIST\",\n    comment=\"Model trained using  XGBEstimator in Snowpark to predict survival of Titanic passengers.\",\n    #metrics={\"accuracy\": model_object.best_score_},\n    conda_dependencies=['xgboost'],\n    version_name='v1'\n)\n\n# View available models\nmodel_registry.show_models()\n\n# View available versions\nmodel_registry.get_model('TITANIC_SURVIVAL_MODEL_SPCS_1').show_versions()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "69a9b068-ac02-4044-aa28-556125642acd",
   "metadata": {
    "language": "python",
    "name": "_7_REGISTER_MODEL2_SPCS",
    "collapsed": false,
    "resultHeight": 0,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "registered_model.create_service(service_name=\"titanic_survival_model_service_1\",service_compute_pool = \"model_compute_pool\",image_repo = \"KAGGLE_TITANIC_CHALLENGE.D_SCH.inference_images\",ingress_enabled=True,gpu_requests=None)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0fea14b5-24ec-4a9c-8e44-4e268a984b31",
   "metadata": {
    "language": "sql",
    "name": "_7_REGISTER_MODEL2_SPCS_ENDPOINTS",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "SHOW ENDPOINTS IN SERVICE titanic_survival_model_service;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2048fdb1-ec84-41e1-ad1e-5ff5d33beb86",
   "metadata": {
    "language": "sql",
    "name": "_7_REGISTER_MODEL2_SPCS_ENDPOINTS_SUSPEND",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "ALTER SERVICE titanic_survival_model_service SUSPEND",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1fee44e4-6bcf-4ae7-8b9a-01aba5125c54",
   "metadata": {
    "language": "python",
    "name": "cell2",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "!pip show snowflake-ml-python",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "afc03526-019b-4a74-b68c-b5f00a7ba22f",
   "metadata": {
    "collapsed": false,
    "name": "_8_SCORING",
    "resultHeight": 74
   },
   "source": [
    "# 8 - Scoring Passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4582065-7f15-4fd8-ba87-f5a17536f8dd",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_8_GENERATE_TEST_DATA_W_FEATURES",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Retrieving features for the test data\n",
    "test_dataset_df = fs.retrieve_feature_values(\n",
    "    spine_df=spine_df_test, \n",
    "    features=[kaggle_fv,custom_fv], \n",
    "    exclude_columns=['SURVIVED']\n",
    ")\n",
    "test_dataset_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "id": "2dc4337a-5a2e-414d-98f8-16194ab44ccb",
   "metadata": {
    "language": "python",
    "name": "_8_PREDICT_SURVIVAL_GET_MODEL_REFERENCE",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "model = model_registry.get_model('TITANIC_SURVIVAL_MODEL').default",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df827e-6a04-47ea-b6e7-a23a6d81eebf",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_8_PREDICT_SURVIVAL",
    "resultHeight": 0
   },
   "outputs": [],
   "source": "# Create and persist predictions from registered model given the retrieved features\npredictions = model.run(test_dataset_df, function_name='predict')\npredictions.write.save_as_table('TITANIC_TEST_PREDICTIONS_WAREHOUSE', mode='overwrite')\n\n# \npredictions = session.table('TITANIC_TEST_PREDICTIONS_WAREHOUSE')\npredictions.show()"
  },
  {
   "cell_type": "code",
   "id": "666a8f84-eb9d-4a03-92db-3854ecaea01e",
   "metadata": {
    "language": "python",
    "name": "_8_PREDICT_SURVIVAL_SPCS",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "service_prediction = model.run(\n    test_dataset_df,\n    function_name=\"predict\",\n    service_name=\"titanic_survival_model_service\")\n\nservice_prediction.write.save_as_table('TITANIC_TEST_PREDICTIONS_SERVICE', mode='overwrite')\n\n# \npredictions_df = session.table('TITANIC_TEST_PREDICTIONS_SERVICE')\npredictions_df.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "74532147-63cc-4b40-8d3b-0c32df4a42ca",
   "metadata": {
    "collapsed": false,
    "name": "_9_LEADERBOARD",
    "resultHeight": 295
   },
   "source": [
    "# 9 - Score your results!\n",
    "Kaggle expects a CSV file with two columns:  \n",
    "* PassengerId\n",
    "* Survived\n",
    "\n",
    "After offloading the data as CSV-file, you can download it via Snowflake's UI.\n",
    "\n",
    "If you don't have a Kaggle Account, you can also evaluate your performance with the following call:  \n",
    "**session.call('calculate_challenge_score', <path-to-your-csv-file>)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b27df-b23e-40b0-a093-d712a0af4c3e",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_9_EXPORT_CSV"
   },
   "outputs": [],
   "source": [
    "# Transform data into format expected by Kaggle\n",
    "kaggle_submission = predictions.select('PASSENGER_ID','SURVIVED_PREDICTION')\n",
    "kaggle_submission = kaggle_submission.with_column_renamed(col('PASSENGER_ID'),'\"PassengerId\"')\n",
    "kaggle_submission = kaggle_submission.with_column_renamed(col('SURVIVED_PREDICTION'),'\"Survived\"')\n",
    "kaggle_submission.show()\n",
    "\n",
    "# Export Predictions to submission.csv\n",
    "kaggle_submission.write.csv(\n",
    "    '@KAGGLE_SUBMISSION/submission.csv', \n",
    "    header=True, \n",
    "    single=True, \n",
    "    format_type_options={\"COMPRESSION\": \"NONE\"},\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a6cc4-f2e0-424e-9684-82e8532b0628",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "_9_CALCULATE_SCORE"
   },
   "outputs": [],
   "source": [
    "session.call('calculate_challenge_score', '@KAGGLE_SUBMISSION/submission.csv')"
   ]
  }
 ]
}